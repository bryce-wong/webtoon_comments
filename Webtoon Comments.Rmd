---
title: "Webtoon Comments"
author: "Bryce Wong"
date: "January 29, 2019"
output: github_document
---

```{r setup, include=FALSE}
library(rvest)
library(stringr)
library(tidyverse)
library(purrr)
library(here)
```

Reading in comments from the first episode:

Some things to note: 

*Will be following ethical principles as outlined [here]("https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01"").

*Will also be adapting how to scrape a javascript page from R Ladies NYC [here]("http://www.rladiesnyc.org/post/scraping-javascript-websites-in-r/#javascript-webscraping-in-r").

*Finally, I have looked at Webtoon's robots.txt and it looks like I should be able to scrape comments.

```{r javascript code}
# write the javascript code to a new file, scrape.js

writeLines("var url = 'http://example.com';
var page = new WebPage();
var fs = require('fs');

page.open(url, function (status) {
        just_wait();
});

function just_wait() {
    setTimeout(function() {
               fs.write('comments.html', page.content, 'w');
            phantom.exit();
    }, 2500);
}
", con = "scrape.js")
```

```{r converting to html}
js_scrape <- function(
                      url = "https://www.webtoons.com/en/challenge/tested/heck-of-a-start/viewer?title_no=231173&episode_no=1", 
                      js_path = "scrape.js"){
  
  # this section will replace the url in scrape.js to whatever you want 
  lines <- readLines(js_path)
  lines[1] <- paste0("var url ='", url ,"';")
  writeLines(lines, js_path)
  
  #command = paste(phantompath, js_path, sep = " ")
  system("phantomjs scrape.js")

}

js_scrape()
```

```{r scrape comments}
# read the newly created html file 
html <- read_html("comments.html")

c <- html %>% 
  html_nodes(".u_cbox_contents") %>%
  html_text()

reviews = tibble(
  comments = c
)
```



